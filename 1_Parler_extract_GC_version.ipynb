{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab version\n",
    "### Note: Google Colab has major issues processing this amount of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and process data into nodes and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bbwzfN7fJ2A6"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "from pprint import pprint\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eT3Uz2tqJT6o",
    "outputId": "9c3f0f6c-caad-4827-bd09-3a38d646039d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "root_path = 'gdrive/My Drive/Parler Data/'  #change dir to your project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jM5jIKDGJgS8"
   },
   "outputs": [],
   "source": [
    "# Written by Alex\n",
    "# unzips the file and goes through it\n",
    "# 1 line is 1 post\n",
    "filename = os.path.join(root_path, 'processed.1850k.jsonl.gz')\n",
    "\n",
    "def iterate_posts():\n",
    "    with gzip.open(filename, 'rb') as fd:\n",
    "      for line in fd:\n",
    "          yield json.loads(line)\n",
    "            \n",
    "gen = iterate_posts()\n",
    "#next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect a set of all usernames found in posts\n",
    "# Using posts.pickle.gz allows 1 line = 1 name\n",
    "# This file breaks replies off from the parent post\n",
    "# so that we don't have to worry about parsing multiple\n",
    "# names in one line.\n",
    "# Once we haved a list of each username from each post,\n",
    "# remove duplicates to get a full list of unique usernames\n",
    "from collections import Counter\n",
    "\n",
    "filename2 = os.path.join(root_path, 'posts.pickle.gz')\n",
    "single_posts = pickle.load(gzip.open(filename2, 'rb'))\n",
    "nodes = []\n",
    "for post in single_posts.values():\n",
    "    nodes.append(post['author'])\n",
    "    \n",
    "user_post_count = Counter(nodes)\n",
    "nodes = set(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data\n",
    "### Everything after here is platform agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@Private User', 37703),\n",
       " ('@ThomasFox', 537),\n",
       " ('@IamfromQ', 448),\n",
       " ('@CounterGlobalist', 408),\n",
       " ('@TommyRobinson', 383),\n",
       " ('@mitchellvii', 339),\n",
       " ('@LibertyElaine', 310),\n",
       " ('@WeLoveTrump', 305),\n",
       " ('@handsomebuster', 303),\n",
       " ('@MIZ37YandMaYh3m', 300)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a dictionary of user post frequencies by username sorted by post frequencies, descending\n",
    "# The sorting isn't really necessary for the next part,\n",
    "# it just makes it easier to spot check manually\n",
    "\n",
    "sorted_user_post_count = sorted(dict(user_post_count).items(), key=lambda item:item[1], reverse=True)\n",
    "sorted_user_post_count[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182811"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of users who only posted a certain number of times\n",
    "# The number of posts may be adjusted to meet filtering needs\n",
    "\n",
    "# CHANGE THIS VARIABLE TO ADJUST USER FILTERING\n",
    "# If minimum posts = 2, there is no need to filter them. \n",
    "# Users with 1 post will be removed when collecting edges\n",
    "minimum_posts = 30 \n",
    "\n",
    "low_posters = [user for user, freq in sorted_user_post_count if freq<minimum_posts]\n",
    "len(low_posters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hyz-m_86oAzB"
   },
   "outputs": [],
   "source": [
    "def get_list_of_users(post):\n",
    "    \"\"\" \n",
    "    Extract all usernames from each post\n",
    "\n",
    "    Note:\n",
    "        There can be None, one, or more usernames in a post.\n",
    "     \n",
    "     Args:\n",
    "         post (dict): Yielded by iterate_posts()\n",
    "\n",
    "    Attributes:\n",
    "        users (dict): Used to collect usernames \n",
    "        \n",
    "    Returns:\n",
    "        users: Tuple of usernames from each post. Can contain any number of usernames.\n",
    "    \"\"\"    \n",
    "\n",
    "    users = []\n",
    "    try:\n",
    "        for post_item in post['posts']:                \n",
    "            users.append(post_item['author_username'])\n",
    "    except KeyError:\n",
    "        return tuple(users) #list of usernames from ONE(1) post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o8Gl5tqEvWrW",
    "outputId": "8a2bf2d2-044c-4af9-d77b-f374214c0aa8"
   },
   "outputs": [],
   "source": [
    "'''# Collect a list of tuples of of length 2 or more usernames from posts\n",
    "# The exception is triggered by reaching the end of the JSONL file.\n",
    "\n",
    "edges_temp=[]\n",
    "all_edges=[]\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        print(\"while true\")\n",
    "        temp_list = get_list_of_users(next(gen))\n",
    "        \n",
    "        for username in temp_list:\n",
    "            print(\"for user in list\")\n",
    "            if username == None:\n",
    "                print('None')\n",
    "                pass\n",
    "            elif username in low_posters:\n",
    "                print('if user in low')\n",
    "                temp_list.remove(username)\n",
    "                low_posters.remove(username)\n",
    "            else:\n",
    "                print('else')\n",
    "                edges_temp.append(username)\n",
    "\n",
    "except:  \n",
    "    print(\"excepting\")\n",
    "    for user_tuple in edges_temp:\n",
    "        if user_tuple != None and len(user_tuple)>1:\n",
    "            all_edges.append(user_tuple)\n",
    "        else:\n",
    "            pass #was continue\n",
    "    del edges_temp #free up memory\n",
    "    print(\"Complete\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect a list of tuples of of length 2 or more usernames from posts\n",
    "# Filters out lists of None and lists with only 1 user. This is why 'minimum_posts' of 2 is unneeded.\n",
    "# The exception is triggered by reaching the end of the JSONL file.\n",
    "def get_edges():\n",
    "    user_tuple_list=[]\n",
    "    edges_temp=[]\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            user_tuple_list.append(get_list_of_users(next(gen))) \n",
    "    except:  \n",
    "        user_tuple_list = list(set(user_tuple_list))\n",
    "        for user_list in user_tuple_list:\n",
    "            if user_list != None and len(user_list)>1:\n",
    "                edges_temp.append(user_list)\n",
    "            else:\n",
    "                continue\n",
    "        print(\"Complete\")\n",
    "        return edges_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_low_posters(all_edges):\n",
    "    temp_list = []\n",
    "    for username_tuple in all_edges:\n",
    "        name_list = []\n",
    "        for username in username_tuple:\n",
    "            if username in low_posters:\n",
    "                low_posters.remove(username)\n",
    "            else: \n",
    "                name_list.append(username)\n",
    "        temp_list.append(name_list)\n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def remove_singles(all_edges):\n",
    "    for user_tuple in all_edges:\n",
    "        if len(user_tuple)<2:\n",
    "            all_edges.remove(user_tuple)\n",
    "        else:\n",
    "            continue'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_edges(all_edges):\n",
    "    \"\"\"  \n",
    "    Split tuples into tuples of length 2 and remove duplicates.\n",
    "\n",
    "    Args:\n",
    "        all_edges (list of tuples): These tuples vary in length.\n",
    "\n",
    "    Attributes: \n",
    "        user_list (list): Keeps tuples generated/passed by for loop.\n",
    "        \n",
    "    Returns:\n",
    "        user_list (list of tuples): Each tuple is of length 2 and duplicates are removed.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    user_list = []\n",
    "    \n",
    "    for user_tuple in all_edges:\n",
    "        if len(user_tuple)==2:\n",
    "            user_list.append(user_tuple)\n",
    "        else:\n",
    "            for username in user_tuple:\n",
    "                if username!=user_tuple[0]:\n",
    "                    user_list.append((user_tuple[0],username))\n",
    "                else:\n",
    "                    continue\n",
    "            continue\n",
    "    return list(set(user_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edges = get_edges()\n",
    "print(f'all_edges is a {type(all_edges)} of {type(all_edges[0])} containing {type(all_edges[0][0])}')\n",
    "len(all_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all elements of edges to be tuples of length 2 with duplicates removed.\n",
    "edges = split_edges(all_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Remove from edges those users who only posted a few times\n",
    "edges = [tup for tup in edges if not any(i in tup for i in low_posters)] '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(nodes)} individual users.')\n",
    "print(f'There are {len(low_posters)} users who made less than {minimum_posts} posts who can be removed.')\n",
    "print(f'That will leave us with {len(edges)} users to graph.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the nodes and edges to a file\n",
    "# TODO: Write code to read in the files to the appropriate variables so that we don't have to run all of the above code again\n",
    "open(os.path.join(root_path,'edges.txt'), 'w').write('\\n'.join('%s %s' % x for x in edges))\n",
    "open(os.path.join(root_path,'nodes.txt'), 'w').write('\\n'.join('%s' % x for x in nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(nodes)} nodes in the graph.')\n",
    "print(f'There are {len(all_edges)} total edges, but this includes duplicates and multiple replies to one post.')\n",
    "print(f'There are {len(edges)} edges once we split the posts and remove duplicates.')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and process the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lda_t41xMEA9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = [G.subgraph(c).copy() for c in nx.connected_components(G)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S=sorted(S, key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=S[1]\n",
    "# S[1] should have 21 nodes\n",
    "# S[0] is the big one that takes forever to plot\n",
    "len(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {sub.number_of_nodes()} nodes, and')\n",
    "print(f'There are {sub.number_of_edges()} edges in the subgraph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(sub, node_size=100, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Degree centrality](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.degree_centrality.html#networkx.algorithms.centrality.degree_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: sort this dict by value for readability\n",
    "dc = nx.degree_centrality(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kernighanâ€“Lin bipartition algorithm](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.kernighan_lin.kernighan_lin_bisection.html#networkx.algorithms.community.kernighan_lin.kernighan_lin_bisection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import kernighan_lin_bisection\n",
    "klb = kernighan_lin_bisection(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: adjust the printing of this for readability\n",
    "klb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Greedy Modularity Community](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.modularity_max.greedy_modularity_communities.html#networkx.algorithms.community.modularity_max.greedy_modularity_communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "gmc = greedy_modularity_communities(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: adjust the printing of this for readability\n",
    "gmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the graph to a GXF file for later use\n",
    "# Can be imported into Gephi\n",
    "nx.write_gexf(sub, \"subgraph.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Parler working.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
