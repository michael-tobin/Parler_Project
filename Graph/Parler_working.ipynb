{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bbwzfN7fJ2A6"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import gzip\n",
    "from pprint import pprint\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If using Google Colab, use the next three cells to import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eT3Uz2tqJT6o",
    "outputId": "9c3f0f6c-caad-4827-bd09-3a38d646039d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "root_path = 'gdrive/My Drive/Parler Data/'  #change dir to your project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "jM5jIKDGJgS8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'root_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3026110c55f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# unzips the file and goes through it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 1 line is 1 post\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'processed.1850k.jsonl.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0miterate_posts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'root_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Written by Alex\n",
    "# unzips the file and goes through it\n",
    "# 1 line is 1 post\n",
    "filename = os.path.join(root_path, 'processed.1850k.jsonl.gz')\n",
    "\n",
    "def iterate_posts():\n",
    "    with gzip.open(filename, 'rb') as fd:\n",
    "      for line in fd:\n",
    "          yield json.loads(line)\n",
    "            \n",
    "gen = iterate_posts()\n",
    "#next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect a set of all usernames found in posts\n",
    "# Using posts.pickle.gz allows 1 line = 1 name\n",
    "# This file breaks replies off from the parent post\n",
    "# so that we don't have to worry about parsing multiple\n",
    "# names in one line.\n",
    "# Once we haved a list of each username from each post,\n",
    "# remove duplicates to get a full list of unique usernames\n",
    "from collections import Counter\n",
    "\n",
    "filename2 = os.path.join(root_path, 'posts.pickle.gz')\n",
    "single_posts = pickle.load(gzip.open(filename2, 'rb'))\n",
    "nodes = []\n",
    "for post in single_posts.values():\n",
    "    nodes.append(post['author'])\n",
    "    \n",
    "user_post_count = Counter(nodes)\n",
    "nodes = set(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If using Jupyter Lab, use the next two cells to import data\n",
    "Assumes that the data files are in the same folder as the .ipynb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by Alex\n",
    "# unzips the file and goes through it\n",
    "# 1 line is 1 post\n",
    "filename = 'processed.1850k.jsonl.gz'\n",
    "def iterate_posts():\n",
    "    with gzip.open(filename, 'rb') as fd:\n",
    "      for line in fd:\n",
    "          yield json.loads(line)\n",
    "            \n",
    "gen = iterate_posts()\n",
    "#next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "teCfv4UmgmA4"
   },
   "outputs": [],
   "source": [
    "# Collect a set of all usernames found in posts\n",
    "# Using posts.pickle.gz allows 1 line = 1 name\n",
    "# This file breaks replies off from the parent post\n",
    "# so that we don't have to worry about parsing multiple\n",
    "# names in one line.\n",
    "# Once we haved a list of each username from each post,\n",
    "# remove duplicates to get a full list of unique usernames\n",
    "from collections import Counter\n",
    "\n",
    "filename2 = 'posts.pickle.gz'\n",
    "single_posts = pickle.load(gzip.open(filename2, 'rb'))\n",
    "nodes = []\n",
    "for post in single_posts.values():\n",
    "    nodes.append(post['author'])\n",
    "    \n",
    "user_post_count = Counter(nodes)\n",
    "nodes = list(set(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data\n",
    "### Everything after here is platform agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dictionary of user post frequencies by username sorted by post frequencies, descending\n",
    "# The sorting isn't really necessary for the next part,\n",
    "# it just makes it easier to spot check manually\n",
    "\n",
    "sorted_user_post_count=sorted(dict(user_post_count).items(), key=lambda item:item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('@Private User', 37703)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_user_post_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get users who only posted a certain number of times\n",
    "# The number of posts may be adjusted to meet filtering needs\n",
    "\n",
    "# CHANGE THIS VARIABLE TO ADJUST USER FILTERING\n",
    "minimum_posts = 4 \n",
    "\n",
    "low_posters = [user for user in sorted_user_post_count if user[1]<minimum_posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hyz-m_86oAzB"
   },
   "outputs": [],
   "source": [
    "def get_edges(post):\n",
    "    \"\"\" \n",
    "    Extract all usernames from each post\n",
    "\n",
    "    Note:\n",
    "        There can be None, one, or more usernames in a post.\n",
    "     \n",
    "     Args:\n",
    "         post (dict): Yielded by iterate_posts()\n",
    "\n",
    "    Attributes:\n",
    "        users (dict): Used to collect usernames \n",
    "        \n",
    "    Returns:\n",
    "        users: Tuple of usernames from each post. Can contain any number of usernames.\n",
    "    \"\"\"    \n",
    "\n",
    "    users = []\n",
    "    try:\n",
    "        for post_item in post['posts']:                \n",
    "            users.append(post_item['author_username'])\n",
    "    except KeyError:\n",
    "        return tuple(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o8Gl5tqEvWrW",
    "outputId": "8a2bf2d2-044c-4af9-d77b-f374214c0aa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# Collect a list of tuples of of length 2 or more usernames from posts\n",
    "# The exception is triggered by reaching the end of the JSONL file.\n",
    "\n",
    "edges_temp=[]\n",
    "all_edges=[]\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        edges_temp.append(get_edges(next(gen))) \n",
    "except:  \n",
    "    edges_temp=list(set(edges_temp))\n",
    "    for user_tuple in edges_temp:\n",
    "        if user_tuple != None and len(user_tuple)>1:\n",
    "            all_edges.append(user_tuple)\n",
    "        else:\n",
    "            continue\n",
    "    print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_edges(all_edges):\n",
    "    \"\"\"  \n",
    "    Split tuples into tuples of length 2 and remove duplicates.\n",
    "\n",
    "    Args:\n",
    "        all_edges (list of tuples): These tuples vary in length.\n",
    "\n",
    "    Attributes: \n",
    "        user_list (list): Keeps tuples generated/passed by for loop.\n",
    "        \n",
    "    Returns:\n",
    "        user_list (list of tuples): Each tuple is of length 2 and duplicates are removed.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    user_list = []\n",
    "    \n",
    "    for user_tuple in all_edges:\n",
    "        if len(user_tuple)==2:\n",
    "            user_list.append(user_tuple)\n",
    "        else:\n",
    "            for username in user_tuple:\n",
    "                if username!=user_tuple[0]:\n",
    "                    user_list.append((user_tuple[0],username))\n",
    "                else:\n",
    "                    continue\n",
    "            continue\n",
    "    return list(set(user_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all elements of edges to be tuples of length 2 with duplicates removed.\n",
    "edges = split_edges(all_edges)\n",
    "split = split_edges(all_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for user_tuple in edges:\n",
    "    if user_tuple[0] not in low_posters:\n",
    "        if user_tuple[1] not in low_posters:\n",
    "            temp.append(user_tuple)\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removal of low posters: 229235\n",
      "After removal of low posters: 229235\n"
     ]
    }
   ],
   "source": [
    "print(f'Before removal of low posters: {len(split)}')\n",
    "print(f'After removal of low posters: {len(temp)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(nodes)} individual users.')\n",
    "print(f'After splitting posts into sets of 2 users, there are {len(split)} sets to check')\n",
    "print(f'There are {len(low_posters)} users who made less than {minimum_posts} posts who can be removed.')\n",
    "print(f'That will leave us with {len(edges2)} sets to graph.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the nodes and edges to a file\n",
    "# TODO: Write code to read in the files to the appropriate variables so that we don't have to run all of the above code again\n",
    "open('edges_with_low_posters_2_removed.txt', 'w').write('\\n'.join('%s %s' % x for x in edges))\n",
    "open('nodes.txt', 'w').write('\\n'.join('%s' % x for x in nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(nodes)} nodes in the graph.')\n",
    "print(f'There are {len(all_edges)} total edges, but this includes duplicates and multiple replies to one post.')\n",
    "print(f'There are {len(edges)} edges once we split the posts and remove duplicates.')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and process the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Lda_t41xMEA9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185207"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228780"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = [G.subgraph(c).copy() for c in nx.connected_components(G)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "S=sorted(S, key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub=S[0]\n",
    "# S[1] should have 21 nodes\n",
    "# S[0] is the big one that takes forever to plot\n",
    "len(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {sub.number_of_nodes()} nodes, and')\n",
    "print(f'There are {sub.number_of_edges()} edges in the subgraph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(sub, node_size=100, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Degree centrality](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.centrality.degree_centrality.html#networkx.algorithms.centrality.degree_centrality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: sort this dict by value for readability\n",
    "dc = nx.degree_centrality(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc['@zvi20']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kernighanâ€“Lin bipartition algorithm](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.kernighan_lin.kernighan_lin_bisection.html#networkx.algorithms.community.kernighan_lin.kernighan_lin_bisection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import kernighan_lin_bisection\n",
    "klb = kernighan_lin_bisection(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: adjust the printing of this for readability\n",
    "klb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Greedy Modularity Community](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.modularity_max.greedy_modularity_communities.html#networkx.algorithms.community.modularity_max.greedy_modularity_communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "gmc = greedy_modularity_communities(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: adjust the printing of this for readability\n",
    "gmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the graph to a GXF file for later use\n",
    "# Can be imported into Gephi\n",
    "nx.write_gexf(sub, \"subgraph.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Parler working.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
